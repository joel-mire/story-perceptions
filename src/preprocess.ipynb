{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmire/story-perceptions/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.88it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from constants import *\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import llama_utils\n",
    "import gpt_utils\n",
    "from llm_prompt_templates import *\n",
    "import survey_utils\n",
    "from collections import defaultdict\n",
    "import metric_utils\n",
    "import pandas_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_dataset_dict = load_dataset(STORY_SEEKER_DATASET_NAME)\n",
    "ss_split_dfs = [pd.DataFrame(ss_dataset_dict[split]) for split in ss_dataset_dict.keys()]\n",
    "ss_df = pd.concat(ss_split_dfs, ignore_index=True)\n",
    "\n",
    "def _format_size(size_bytes):\n",
    "    \"\"\"Convert bytes to human-readable format\"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "        if size_bytes < 1024.0:\n",
    "            return f\"{size_bytes:.2f} {unit}\"\n",
    "        size_bytes /= 1024.0\n",
    "    return f\"{size_bytes:.2f} TB\"\n",
    "\n",
    "def download_with_progress(url, chunk_size=8192):\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    start_time = time.time()\n",
    "    downloaded_size = 0\n",
    "    data = io.BytesIO()\n",
    "    with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading\") as pbar:\n",
    "        for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "            size = data.write(chunk)\n",
    "            downloaded_size += size\n",
    "            pbar.update(size)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time > 0:\n",
    "                speed = downloaded_size / elapsed_time\n",
    "                pbar.set_postfix(speed=f\"{_format_size(speed)}/s\", refresh=True)\n",
    "    return data\n",
    "\n",
    "if not os.path.exists(TLDR_SS_SUBSET_PATH) or FORCE_REHYDRATE_TLDR:\n",
    "    ss_ids = set(ss_df['id'].values)\n",
    "    tldr_ss_subset_dict = defaultdict(dict)\n",
    "\n",
    "    print(\"Starting download...\")\n",
    "    tldr_zip_data = download_with_progress(TLDR_URL)\n",
    "\n",
    "    print(\"Processing TLDR_17 ZIP file contents...\")\n",
    "    with ZipFile(tldr_zip_data, 'r') as zip_obj:\n",
    "        print(\"ZIP file opened successfully\")\n",
    "        for _filename in zip_obj.namelist():\n",
    "            with zip_obj.open(_filename) as _file:\n",
    "                for _line in tqdm(_file):\n",
    "                    _data = json.loads(_line)\n",
    "                    if _data['id'] in ss_ids:\n",
    "                        tldr_ss_subset_dict[_data['id']] = _data\n",
    "\n",
    "    print(\"Saving TLDR_17 instances that intersect with StorySeeker...\")\n",
    "    tldr_ss_subset_df = pd.DataFrame.from_dict(tldr_ss_subset_dict, orient='index')\n",
    "    tldr_ss_subset_df.to_csv(TLDR_SS_SUBSET_PATH, index=False)\n",
    "else:\n",
    "    tldr_ss_subset_df = pd.read_csv(TLDR_SS_SUBSET_PATH)\n",
    "\n",
    "# data is small enough to work with in dictionary format for convenience\n",
    "tldr_ss_subset_dict = tldr_ss_subset_df.set_index('id').T.to_dict()\n",
    "ss_df['text'] = ss_df['id'].apply(lambda id: tldr_ss_subset_dict[id]['body'])\n",
    "# ss_df.to_csv(SS_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcr_df count: 2496\n"
     ]
    }
   ],
   "source": [
    "pcr_df = pd.read_csv(POTATO_CODED_RAW_FILTERED_PATH, converters=pandas_utils.get_list_converters(POTATO_CODED_RAW_FILTERED_PATH))\n",
    "print('pcr_df count:', len(pcr_df.index))\n",
    "sse_df = ss_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_abrev, model in [('llama3', ''),\n",
    "                           ('gpt4o','gpt-4o-2024-05-13'),\n",
    "                           ('gpt4t', 'gpt-4-turbo-2024-04-09'),\n",
    "                           ('gpt4','gpt-4-0613')]:\n",
    "  for prompt_idx, prompt_template in enumerate([TEXT_TO_DECISION_AND_RATIONALE_0,\n",
    "                                                TEXT_TO_DECISION_AND_RATIONALE_1,\n",
    "                                                TEXT_TO_DECISION_AND_RATIONALE_2,\n",
    "                                                TEXT_TO_DECISION_AND_RATIONALE_3,\n",
    "                                                TEXT_TO_DECISION_AND_RATIONALE_4]):\n",
    "    if 'gpt' in model_abrev and (f'{model_abrev}_descriptive_label_{prompt_idx}' not in sse_df.columns or FORCE_RERUN_GPT):\n",
    "      output_path = f'{GPT_RESULTS_DIR}/{model_abrev}/text_to_decision_and_rationale_{prompt_idx}.csv'\n",
    "      sse_df = gpt_utils.process(df=sse_df,\n",
    "                                prompt_template=prompt_template,\n",
    "                                var_col_dict={\"[TEXT]\": 'text'},\n",
    "                                var_val_dict={},\n",
    "                                output_path=output_path,\n",
    "                                force_rerun=FORCE_RERUN_GPT,\n",
    "                                model=model,\n",
    "                                model_abrev=model_abrev)\n",
    "      sse_df.to_csv(SS_EXTENDED_PATH, index=False)\n",
    "\n",
    "    elif 'llama3' in model_abrev and (f'{model_abrev}_descriptive_label_{prompt_idx}' not in sse_df.columns or FORCE_RERUN_LLAMA3):\n",
    "      output_path = f'{LLAMA3_RESULTS_DIR}/text_to_decision_and_rationale_{prompt_idx}.csv'\n",
    "      sse_df = llama_utils.process(df=sse_df,\n",
    "                                    prompt_template=prompt_template,\n",
    "                                    var_col_dict={\"[TEXT]\": 'text'},\n",
    "                                    var_val_dict={},\n",
    "                                    output_path=output_path,\n",
    "                                    idx=prompt_idx,\n",
    "                                    force_rerun=FORCE_RERUN_LLAMA3,\n",
    "                                    model_abrev=model_abrev)\n",
    "      sse_df.to_csv(SS_EXTENDED_PATH, index=False)\n",
    "      \n",
    "  sse_df = pd.read_csv(SS_EXTENDED_PATH)\n",
    "  \n",
    "  cols = [f'{model_abrev}_descriptive_label_{prompt_idx}' for prompt_idx in range(5)]\n",
    "  sse_df[f'{model_abrev}_descriptive_label_mv'] = sse_df[cols].mode(axis=1)[0]\n",
    "  sse_df[f'{model_abrev}_descriptive_label_union'] = sse_df[cols].max(axis=1)\n",
    "\n",
    "  sse_df.to_csv(SS_EXTENDED_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_df = pcr_df.copy()\n",
    "\n",
    "pc_df['confidence'] = pc_df.apply(lambda row: survey_utils.get_confidence_score(row), axis=1)\n",
    "pc_df['familiarity'] = pc_df.apply(lambda row: survey_utils.get_familiarity_score(row), axis=1)\n",
    "pc_df['label'] = pc_df.apply(lambda row: survey_utils.get_label(row), axis=1)\n",
    "pc_df['gc_label'] = pc_df['instance_id'].apply(lambda instance_id: sse_df.loc[sse_df['id'] == instance_id, 'gold_consensus'].iloc[0])\n",
    "pc_df['gpt4_descriptive_label_mv'] = pc_df['instance_id'].apply(lambda instance_id: sse_df.loc[sse_df['id'] == instance_id, 'gpt4_descriptive_label_mv'].iloc[0])\n",
    "pc_df['gpt4t_descriptive_label_mv'] = pc_df['instance_id'].apply(lambda instance_id: sse_df.loc[sse_df['id'] == instance_id, 'gpt4t_descriptive_label_mv'].iloc[0])\n",
    "pc_df['gpt4o_descriptive_label_mv'] = pc_df['instance_id'].apply(lambda instance_id: sse_df.loc[sse_df['id'] == instance_id, 'gpt4o_descriptive_label_mv'].iloc[0])\n",
    "pc_df['llama3_descriptive_label_mv'] = pc_df['instance_id'].apply(lambda instance_id: sse_df.loc[sse_df['id'] == instance_id, 'llama3_descriptive_label_mv'].iloc[0])\n",
    "\n",
    "pc_df['goal'] = pc_df['goal:::text_box']\n",
    "pc_df['goal_codes'] = pc_df['goal_codes'].apply(survey_utils.get_code_list)\n",
    "pc_df['rationale'] = pc_df['story_decision_explanation:::text_box']\n",
    "pc_df['rationale_codes'] = pc_df['story_decision_explanation_codes'].apply(survey_utils.get_code_list)\n",
    "pc_df['alternative'] = pc_df['story_alternative:::text_box']\n",
    "pc_df['alternative_codes'] = pc_df['story_alternative_codes'].apply(survey_utils.get_code_list)\n",
    "pc_df['is_coded'] = pc_df.apply(lambda row: survey_utils.instance_coded(row), axis=1)\n",
    "pc_df = pc_df[['user', 'instance_id', 'confidence', 'familiarity', 'label', 'gc_label', 'gpt4_descriptive_label_mv', 'gpt4t_descriptive_label_mv', 'gpt4o_descriptive_label_mv', 'llama3_descriptive_label_mv', 'goal', 'goal_codes', 'rationale', 'rationale_codes', 'alternative', 'alternative_codes', 'is_coded']]\n",
    "\n",
    "pc_df = pc_df[pc_df['is_coded'] == True]\n",
    "pc_df.to_csv(POTATO_CODED_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sp_df count: 502\n"
     ]
    }
   ],
   "source": [
    "default_dict_for_instance = lambda: defaultdict(list)\n",
    "instance_crowd_dict = defaultdict(default_dict_for_instance)\n",
    "for i, row in pc_df.iterrows():\n",
    "    instance_id = row['instance_id']\n",
    "    confidence = row['confidence']\n",
    "    familiarity = row['familiarity']\n",
    "    label = row['label']\n",
    "    instance_crowd_dict[instance_id]['crowd_confidence_scores'].append(confidence)\n",
    "    instance_crowd_dict[instance_id]['crowd_familiarity_scores'].append(familiarity)\n",
    "    instance_crowd_dict[instance_id]['crowd_labels'].append(label)\n",
    "    instance_crowd_dict[instance_id]['crowd_goals'].append(row['goal'])\n",
    "\n",
    "instance_crowd_df = pd.DataFrame.from_dict(instance_crowd_dict).transpose()\n",
    "instance_crowd_df['crowd_confidence_avg'] = instance_crowd_df['crowd_confidence_scores'].apply(lambda x: metric_utils.get_avg(x))\n",
    "instance_crowd_df['crowd_familiarity_avg'] = instance_crowd_df['crowd_familiarity_scores'].apply(lambda x: metric_utils.get_avg(x))\n",
    "instance_crowd_df['crowd_label_union'] = instance_crowd_df['crowd_labels'].apply(lambda x: 1 if 1 in x else 0)\n",
    "instance_crowd_df['crowd_label_mv'] = instance_crowd_df['crowd_labels'].apply(lambda x: metric_utils.get_majority_vote(x))\n",
    "instance_crowd_df['crowd_label_mv_rate'] = instance_crowd_df['crowd_labels'].apply(lambda x: metric_utils.get_majority_vote_rate(x))\n",
    "instance_crowd_df['crowd_confidence_entropy'] = instance_crowd_df['crowd_confidence_scores'].apply(metric_utils.get_entropy)\n",
    "instance_crowd_df['crowd_familiarity_entropy'] = instance_crowd_df['crowd_familiarity_scores'].apply(metric_utils.get_entropy)\n",
    "instance_crowd_df['crowd_labels_entropy'] = instance_crowd_df['crowd_labels'].apply(metric_utils.get_entropy)\n",
    "\n",
    "sp_df = pd.merge(sse_df, instance_crowd_df, how='left', left_on='id', right_index=True)\n",
    "sp_df.to_csv(SP_PATH)\n",
    "print('sp_df count:', len(sp_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_df['crowd_confidence_avg'] = pc_df['instance_id'].apply(lambda instance_id: sp_df.loc[sp_df['id'] == instance_id, 'crowd_confidence_avg'].iloc[0])\n",
    "pc_df['crowd_label_mv'] = pc_df['instance_id'].apply(lambda instance_id: sp_df.loc[sp_df['id'] == instance_id, 'crowd_label_mv'].iloc[0])\n",
    "pc_df['crowd_label_mv_rate'] = pc_df['instance_id'].apply(lambda instance_id: sp_df.loc[sp_df['id'] == instance_id, 'crowd_label_mv_rate'].iloc[0])\n",
    "pc_df['crowd_labels_entropy'] = pc_df['instance_id'].apply(lambda instance_id: sp_df.loc[sp_df['id'] == instance_id, 'crowd_labels_entropy'].iloc[0])\n",
    "pc_df['crowd_confidence_entropy'] = pc_df['instance_id'].apply(lambda instance_id: sp_df.loc[sp_df['id'] == instance_id, 'crowd_confidence_entropy'].iloc[0])\n",
    "pc_df.to_csv(POTATO_CODED_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
